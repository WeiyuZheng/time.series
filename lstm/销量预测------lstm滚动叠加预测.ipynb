{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 销量预测-----lstm添加滚动预测\n",
    "说明:按照序列预测，每次均是预测下个月数据，未知数据用上次预测结果填充，模型前序均为真实值的模型；\n",
    "    例如，预测已知201401~201811月数据，预测201812~201902的值。\n",
    "    根据201401~201811月数据训练出模型model，预测出201812值；\n",
    "    然后将201812预测值当作真实值放进预测模型，再次利用已训练出的model预测出201901的值；\n",
    "    然后将201812、201901预测值当作真实值，利用上述model预测出201902的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "import datetime\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" \n",
    "\n",
    "pd.set_option('precision',4) #设置精度\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)  \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "# keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  把序列数据转化为监督学习模式：X和y一起存放在一张宽表中,n_out为预测滞后第几期\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    df = pd.DataFrame(data)\n",
    "    n_vars = data.shape[1]\n",
    "    cols, names = list(), list()\n",
    "    \n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    \n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型训练函数\n",
    "def model_train(data_train,n_hours,n_batch, nb_epoch, n_neurons, dropout,n_features):\n",
    "    \n",
    "    values = data_train.values.astype('float32')\n",
    "    n_train_hours=data_train.shape[0]-1\n",
    "    train_values=values[:n_train_hours]\n",
    "    \n",
    "    # 拆分训练集、验证集(验证集为训练集最后一个点)\n",
    "    train = train_values[:n_train_hours, :]\n",
    "    test = train_values[(n_train_hours-n_hours-1):n_train_hours, :]\n",
    "    \n",
    "    # 变量标准化\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_train = scaler.fit_transform(train)\n",
    "    scaled_test = scaler.transform(test)\n",
    " \n",
    "    # 序列数据转化为监督数据类型\n",
    "    reframed_train = series_to_supervised(scaled_train, n_hours).values\n",
    "    reframed_test = series_to_supervised(scaled_test, n_hours).values\n",
    " \n",
    "    # 准备train_X,train_y,test_X,test_y  \n",
    "    n_obs = n_hours * n_features\n",
    "    train_X, train_y = reframed_train[:, :n_obs], reframed_train[:, -1]\n",
    "    test_X, test_y = reframed_test[:, :n_obs], reframed_test[:, -1]\n",
    "    # reshape train_X和test_X成3D数组格式 [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\n",
    "    test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n",
    "    \n",
    "    \n",
    "    # 设计LSTM网络\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_neurons, input_shape=(train_X.shape[1], train_X.shape[2]),dropout=dropout))\n",
    "    model.add(Dense(1))  #n_dim 输出预测个数\n",
    "   \n",
    "    # 模型编译\n",
    "    model.compile(loss='mape', optimizer='adam')\n",
    "    \n",
    "    #模型训练\n",
    "    model_history = model.fit(train_X, train_y, epochs=nb_epoch, batch_size=n_batch, validation_data=(test_X, test_y), verbose=0, shuffle=False)\n",
    "    \n",
    "    # 取每一轮预测的最后一个训练数值\n",
    "    train_y = model.predict(train_X)\n",
    "        \n",
    "        \n",
    "    #y预测值：归一化结果反转化\n",
    "    if  n_features==1:\n",
    "        train_Y=scaler.inverse_transform(train_y)[0]\n",
    "    elif n_features>1:\n",
    "        train_X = train_X.reshape((train_X.shape[0], n_hours*n_features))\n",
    "        train_Y = np.concatenate((train_X[:,-n_features:-1],train_y),axis=1) #拼接成设置scaler的数据格式  #要改成变量个数\n",
    "        train_Y=train_Y.reshape(train_Y.shape[0],n_features)\n",
    "        train_Y = scaler.inverse_transform(train_Y)\n",
    "        train_Y=train_Y[-11:,-1:] #每组取最后十二个预测值\n",
    "    \n",
    "    # inv_yhat = inv_yhat[:,-1] #逆转换完成之后，只需要取第1个特征（yhat）即可\n",
    "    train_Y=pd.DataFrame(train_Y) \n",
    "    \n",
    "    return train_Y,scaler,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##模型预测函数\n",
    "def model_predict(predict_values,n_hours,scaler,model,n_features):\n",
    "    predict_values=pd.DataFrame(predict_values).fillna(1800000).values  #结果不受填充值影响\n",
    "    # 变量标准化\n",
    "    scaled_predict = scaler.transform(predict_values)\n",
    "\n",
    "    # 序列数据转化为监督数据类型\n",
    "    reframed_predict = series_to_supervised(scaled_predict, n_hours).values\n",
    "\n",
    "    \n",
    "    # 准备train_X,train_y,test_X,test_y\n",
    "    n_obs = n_hours * n_features\n",
    "    predict_X1, predict_y = reframed_predict[:, :n_obs], reframed_predict[:, -1]\n",
    "    # reshape train_X和test_X成3D数组格式 [samples, timesteps, features]\n",
    "    predict_X = predict_X1.reshape((predict_X1.shape[0], n_hours, n_features))\n",
    " \n",
    "    # 预测数据\n",
    "    yhat = model.predict(predict_X)\n",
    "\n",
    "    #y预测值：归一化结果反转化\n",
    "    if  n_features==1:\n",
    "        inv_yhat=scaler.inverse_transform(yhat)[0]\n",
    "    elif n_features>1:\n",
    "        inv_yhat =   np.concatenate((predict_X1[0][-n_features:-1],yhat[0]),axis=0) #拼接成设置scaler的数据格式\n",
    "        inv_yhat=inv_yhat.reshape(1,n_features)\n",
    "        inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "        inv_yhat = inv_yhat[:,-1] #逆转换完成之后，只需要取第1个特征（yhat）即可\n",
    "\n",
    "    predict_data=list(inv_yhat)\n",
    "    \n",
    "    return predict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##模型预测函数调用\n",
    "def model_predict_inter(data_predict,n_inter,model,scaler,n_hours,n_features): \n",
    "    compare_data_origin=[]\n",
    "    values = data_predict.values.astype('float32')\n",
    "    predict_values=values[(data_predict.shape[0]-1-n_hours):(data_predict.shape[0])]\n",
    "    for m in range(n_inter):\n",
    "        compare=model_predict(predict_values,n_hours,scaler,model,n_features)\n",
    "        compare_data_origin=compare if compare_data_origin==[] else compare_data_origin+compare        \n",
    "    return compare_data_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_scroll_predict(var_list,predict_data_index,list_date,n_inter):\n",
    "    ###模型参数\n",
    "    n_batch=64\n",
    "    nb_epoch=100\n",
    "    n_neurons=100  #神经元的数量\n",
    "    dropout=0.1\n",
    "    n_hours=12\n",
    "\n",
    "    dataset_fill_predict=dataset[var_list]\n",
    "    var_target=var_list[-1]\n",
    "    step_list_total=pd.DataFrame()\n",
    "    n_features=len(var_list)\n",
    "    for i in list_date:\n",
    "        predict_date=predict_data_index[predict_data_index.index(i):(predict_data_index.index(i)+3)]\n",
    "        step_list=[]\n",
    "        for year_value in range(3):\n",
    "            if year_value==0:\n",
    "                data_predict= dataset_fill_predict.loc[pd.date_range('2014-01-01',predict_date[0],freq='MS'),:]\n",
    "                #模型训练\n",
    "                train_Y,scaler,model=model_train(data_predict,n_hours,n_batch, nb_epoch, n_neurons, dropout,n_features)\n",
    "                #模型预测\n",
    "                year_value_0=model_predict_inter(data_predict,n_inter,model,scaler,n_hours,n_features)\n",
    "                year_value_0=pd.Series(year_value_0).mean()\n",
    "                step_list=[year_value_0]\n",
    "            elif year_value==1:\n",
    "                data_predict= dataset_fill_predict.loc[pd.date_range('2014-01-01',predict_date[1],freq='MS'),:]\n",
    "                data_predict.loc[predict_date[0],var_target]=year_value_0\n",
    "                year_value_1=model_predict_inter(data_predict,n_inter,model,scaler,n_hours,n_features)\n",
    "                year_value_1=pd.Series(year_value_1).mean()\n",
    "                step_list=step_list+[year_value_1]\n",
    "            elif year_value==2:\n",
    "                data_predict= dataset_fill_predict.loc[pd.date_range('2014-01-01',predict_date[2],freq='MS'),:]\n",
    "                data_predict.loc[predict_date[0],var_target]=year_value_0\n",
    "                data_predict.loc[predict_date[1],var_target]=year_value_1\n",
    "                year_value_2=model_predict_inter(data_predict,n_inter,model,scaler,n_hours,n_features)\n",
    "                year_value_2=pd.Series(year_value_2).mean()\n",
    "                step_list=step_list+[year_value_2]\n",
    "        step_list_total=pd.Series(step_list) if step_list_total.empty else pd.concat([step_list_total,pd.Series(step_list)],axis=1)\n",
    "    step_list_total=step_list_total.T\n",
    "    step_list_total.index=list_date\n",
    "    print(step_list_total)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测变量 ['month', 'work_days', 'days', 'y', 'y']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-17068949eddf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscroll_predict_var\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'预测变量'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mlstm_scroll_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredict_data_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlist_date\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_inter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-564b2358883b>\u001b[0m in \u001b[0;36mlstm_scroll_predict\u001b[1;34m(var_list, predict_data_index, list_date, n_inter)\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0mtrain_Y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_predict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_hours\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neurons\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[1;31m#模型预测\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                 \u001b[0myear_value_0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_predict_inter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_predict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_inter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_hours\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                 \u001b[0myear_value_0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear_value_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[0mstep_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0myear_value_0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-ccf5a31395da>\u001b[0m in \u001b[0;36mmodel_predict_inter\u001b[1;34m(data_predict, n_inter, model, scaler, n_hours, n_features)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mpredict_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_predict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mn_hours\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_predict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_inter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mcompare\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_hours\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mcompare_data_origin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompare\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcompare_data_origin\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mcompare_data_origin\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcompare\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcompare_data_origin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_predict' is not defined"
     ]
    }
   ],
   "source": [
    "predict_data_index=list(pd.date_range('2014-01-01','2020-12-01',freq='MS'))  #时间序列索引，已写的很大，可不修改\n",
    "# 读取数据\n",
    "dataset = pd.read_csv('predict_data_final.csv', header=0, index_col=0)\n",
    "dataset.index=pd.to_datetime(dataset.index)\n",
    "#dataset.index=pd.date_range('2014-01-01','2019-02-01',freq='MS')  \n",
    "\n",
    "#列表四个值分别为总量、细分市场（合资、自主豪华）预测\n",
    "scroll_predict_var=[['month','work_days','days','y','y']\n",
    "                   ,['month','work_days','days','y_joint','y_joint']\n",
    "                   ,['month','work_days','days','y_independent','y_independent']\n",
    "                   ,['month','work_days','days','y_luxury','y_luxury']]\n",
    "\n",
    "n_inter=2  #结果循环遍数\n",
    "#预测月份\n",
    "list_date=list(pd.date_range('2018-01-01',(datetime.date.today().replace(day=1) - datetime.timedelta(1)).replace(day=1).strftime(\"%Y-%m-%d\"),freq='MS')) \n",
    "\n",
    "for i in scroll_predict_var:\n",
    "    print('预测变量',i)\n",
    "    lstm_scroll_predict(i,predict_data_index,list_date,n_inter)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
